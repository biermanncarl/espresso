{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ideal-decline",
   "metadata": {},
   "source": [
    "# Tutorial: Error Estimation - Part 2 (Autocorrelation Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-investor",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "0. [Data generation](#Data-generation)\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Computing the auto-covariance function](#Computing-the-auto-covariance-function)\n",
    "3. [Autocorrelation time](#Autocorrelation-time)\n",
    "4. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-shanghai",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "This first code cell will provide us with the same two data sets as in the previous part of this tutorial. We will use them to get familiar with the autocorrelation analysis method of error estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate simulation data using AR(1) processes\n",
    "\n",
    "print(\"Generating data sets for the tutorial ...\\n\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(43)\n",
    "\n",
    "def ar_1_process(n_samples, y0, c, phi, eps, n_warmup):\n",
    "    y = y0\n",
    "    for i in range(n_warmup):\n",
    "        y = c + phi*y + eps*np.random.uniform(-1.0,1.0)\n",
    "    ys = np.zeros(n_samples)\n",
    "    ys[0] = y\n",
    "    for i in range(1,n_samples):\n",
    "        ys[i] = c + phi*ys[i-1] + eps*np.random.uniform(-1.0,1.0)\n",
    "    return ys\n",
    "\n",
    "N_SAMPLES = 100000\n",
    "\n",
    "time_series_1 = ar_1_process(N_SAMPLES, 0.0, 2.0, 0.85, 2.0, 100)\n",
    "time_series_2 = ar_1_process(N_SAMPLES, 0.0, 0.05, 0.995, 1.0, 1000)\n",
    "\n",
    "\n",
    "plt.title(\"The first 1000 samples of both time series\")\n",
    "plt.plot(time_series_1[0:1000], label=\"time series 1\")\n",
    "plt.plot(time_series_2[0:1000], label=\"time series 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-training",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the first part of the error analysis tutorial we have introduced the binning analysis, an easy and common tool for error estimation. However, we have seen that it failed to deliver an estimate for our second data set. In this tutorial, we will get to know a different method: the *autocorrelation analysis*, sometimes also called *auto covariance analysis*. It not only delivers an estimate for the standard error of the mean (SEM), but also information on the correlations and the optimal sampling rate.\n",
    "\n",
    "Before we start computing anything, we will give a brief overview over the relevant quantities and how they relate to each other. This outlines how one would go about computing these quantities. The end goal of this process is to define an estimate for the standard error of the mean $\\sigma_\\overline{X}$. And if the data allows for it, it can be calculated. If it fails, autocorrelation analysis provides more insight into the causes of the faliure than the binning analysis from the first part of this tutorial. Albeit being more involved, it can provide a valuable tool for systems with difficult statistics.\n",
    "\n",
    "Let us begin the theory by defining the auto-covariance function $R^{XX}(\\tau)$ of an observable $X$, at lag time $\\tau$:\n",
    "$\n",
    "\\begin{align}\n",
    "    R^{XX}(\\tau) &\\equiv \\langle (X(t)-\\langle X \\rangle)(X(t+\\tau)-\\langle X \\rangle) \\rangle \\\\\n",
    "    &= \\langle X(t) X(t+\\tau) \\rangle - \\langle X \\rangle^2, \\tag{1}\n",
    "\\end{align}\n",
    "$\n",
    "where $\\langle \\dots \\rangle$ denotes the ensemble average of the expression inside the angled brackets â€” e.g. $\\langle X \\rangle$ is the true mean value of the observable $X$. In the previous part we have established an understanding of correlations as being the \"similarity\" of successive samples. This is an intuitive but inaccurate understanding. The auto-covariance function provides a means to measure and quantify correlation.\n",
    "\n",
    "Computing the auto-covariance for $\\tau=0$ yields the variance $\\sigma=\\langle X^2 \\rangle - \\langle X \\rangle^2$. Normalizing the auto-covariance function by the variance yields the *autocorrelation function* (ACF)\n",
    "$\n",
    "\\begin{align}\n",
    "    A^{XX}(\\tau) = \\frac{R^{XX}(\\tau)}{R^{XX}(0)} = \\frac{\\langle X(t) X(t+\\tau) \\rangle - \\langle X \\rangle^2}{\\langle X^2 \\rangle - \\langle X \\rangle^2}. \\tag{2}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "The ACF can be used to estimate the correlation time $\\tau_X$. Often, this can be simply done by fitting an exponential function to $A^{XX}$, for which we receive  $\\tau_{X, \\mathrm{exp}}$ as the inverse decay rate. However, the ACF doesn't necessarily assume the shape of an exponential. That is when another quantity, called the *integrated autocorrelation time* \n",
    "$\n",
    "\\begin{align}\n",
    "    \\tau_{X, \\mathrm{int}} \\equiv \\int_0^\\infty A^{XX}(\\tau) \\mathrm{d}\\tau \\tag{3}\n",
    "\\end{align}\n",
    "$\n",
    "comes into play. Those two correlation times $\\tau_{X, \\mathrm{int}}$ and $\\tau_{X, \\mathrm{exp}}$ are identical for exponential ACFs, but if the ACF isn't exponential, $\\tau_{X, \\mathrm{int}}$ is the only meaningful quantity. It is related to the effective number of samples\n",
    "$\n",
    "\\begin{align}\n",
    "    N_\\mathrm{eff} = \\frac{N}{2 \\tau_{X, \\mathrm{int}}} \\tag{4}\n",
    "\\end{align}\n",
    "$\n",
    "and also to the standard error of the mean (SEM)\n",
    "$\n",
    "\\begin{align}\n",
    "    \\sigma_\\overline{X} = \\sqrt{\\frac{2 \\sigma_X^2 \\tau_{X, \\mathrm{int}}}{N}} = \\sqrt{\\frac{\\sigma_X^2}{N_\\mathrm{eff}}}. \\tag{5}\n",
    "\\end{align}\n",
    "$\n",
    "where $\\sigma_X^2 = \\langle X^2 \\rangle-\\langle X \\rangle ^2$ is the variance of the observable $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-talent",
   "metadata": {},
   "source": [
    "## Computing the auto-covariance function\n",
    "\n",
    "Equations (1) and (2) involve an infinite, continuous time series $X(t)$. In the simulation bisuness however, we work with finite, discrete time series. These limitations dictate how we can estimate the true (unknown) autocorrelation function. For a finite, time-discrete set of samples $X_i$, a commonly used estimator is the following expression\n",
    "$\n",
    "\\begin{align}\n",
    "    \\hat{R}^{XX}_j = \\frac{1}{N} \\sum^{N-|j|}_{i=1}(X_i-\\overline{X})(X_{i+|j|}-\\overline{X}), \\tag{6}\n",
    "\\end{align}\n",
    "$\n",
    "where $N$ is the total number of samples, and $\\overline{X}=\\frac{1}{N}\\sum_{i=1}^N X_i$ is the average of all samples. This estimates the auto-covariance function at lag time $\\tau=j\\Delta t$ where $\\Delta t$ is the time separation between samples.\n",
    "\n",
    "Before we continue, we want to notify the reader about a few subtleties about this estimator in equation (6).\n",
    "* Ideally, we would use $\\langle X \\rangle$ instead of $\\overline{X}$, since the latter is only an estimate of the former. In most cases we don't know $\\langle X \\rangle$, thus we introduce a small unknown bias by using the estimated mean $\\overline{X}$ instead.\n",
    "* Actually, the sum does not contain $N$ terms, but $N-|j|$ terms. Consequently, we should divide the whole sum by $N-|j|$ and not by $N$. In fact, this approach yields a different estimator to the auto-covariance function (the so-called *unbiased* estimator). However, for large $N$ and small $j$, both estimators yield similar results. This is why the simpler $N$ is commonly used anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-canal",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise\n",
    "Compute the auto-covariance function of the data in `time_series_1` using the estimator in equation (6) and store it into a numpy array called `autocov`. Compute it for all $j$ from `0` up to `999`. Plot it against $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-serve",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "```python\n",
    "# naive Python solution\n",
    "autocov = np.zeros(1000)\n",
    "avg = np.average(time_series_1)\n",
    "for j in range(1000):\n",
    "    temp = 0\n",
    "    for i in range(N_SAMPLES-j):\n",
    "        temp += (time_series_1[i]-avg)*(time_series_1[i+j]-avg)\n",
    "    autocov[j] = temp / N_SAMPLES\n",
    "\n",
    "plt.plot(autocov)\n",
    "plt.xlabel(\"lag time\")\n",
    "plt.ylabel(\"auto-covariance\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-reading",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "arabic-puzzle",
   "metadata": {},
   "source": [
    "Depending on your implementation, this computation might have taken a significant amount of time (up to a couple tens of seconds). When doing a lot of these computations, using highly optimized routines for numerics can save a lot of time. The following example shows how to utilize the common Numpy package to do the job quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy solution\n",
    "time_series_1_centered = time_series_1 - np.average(time_series_1)\n",
    "autocov = np.empty(1000)\n",
    "\n",
    "for j in range(1000):\n",
    "    autocov[j] = np.dot(time_series_1_centered[:N_SAMPLES-j], time_series_1_centered[j:])\n",
    "autocov /= N_SAMPLES\n",
    "\n",
    "plt.gca().axhline(0, color=\"gray\",linewidth=1)\n",
    "plt.plot(autocov)\n",
    "plt.xlabel(\"lag time\")\n",
    "plt.ylabel(\"auto-covariance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-speech",
   "metadata": {},
   "source": [
    "We can see that the auto-covariance function starts at a high value and decreases quickly into a long noisy tail which fluctuates around zero. The high values at short lag times indicate that there are strong correlations at short time scales, as expected. However, even though the tail looks uninteresting, it can bear important information about the statistics of your data. Small systematic deviations from 0 in the tail can be a hint that long-term correlations exist in your system. On the other hand, if there is no sign of a systematic deviation from 0 in the tail, this usually means that the correlation is decaying well within the simulation time, and that the statistics are good enough to estimate an error. In the above example, the correlation quickly decays to zero. Despite the noise in the tail, the statistics seem very reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-deficit",
   "metadata": {},
   "source": [
    "## Autocorrelation time\n",
    "\n",
    "Continuing our example, we can zoom into the first part of the auto-covariance function (using a log scale). We see that it indeed does have similarities with an exponential decay curve. In general, it isn't an exponential, but often can be approximated using one. If it matches reasonably well, the inverted prefactor in the exponential can be directly used as the *correlation time*, which is a measure for how many sampling intervals it takes for correlations to decay. Execute the following code cell for an illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-graphic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def exp_fnc(x,a,b):\n",
    "    return a*np.exp(-x/b)\n",
    "\n",
    "N_MAX = 1000\n",
    "j = np.linspace(0,N_MAX-1,N_MAX)\n",
    "popt, pcov = curve_fit(exp_fnc, j, autocov[:N_MAX], p0=[15,10])\n",
    "\n",
    "plt.plot(j,autocov[:N_MAX])\n",
    "plt.plot(j,exp_fnc(j,popt[0],popt[1]))\n",
    "plt.xlim((1,N_MAX))\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"lag time\")\n",
    "plt.ylabel(\"auto-covariance\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Exponential autocorrelation time: {:.2f} sampling intervals\".format(popt[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-knife",
   "metadata": {},
   "source": [
    "Since the auto-covariance function is very well matched with an exponential, this analysis already gives us a reasonable estimate of the autocorrelation time. For the sake of completeness, we also want to compute the integrated correlation time. For that purpose, we first need to normalize the auto-covariance function in order to get the autocorrelation function, and then integrate over it.\n",
    "\n",
    "The integration in equation (3) is again approximated as a discrete sum over the first $j_\\mathrm{max}$ values of the ACF:\n",
    "$\n",
    "\\hat{\\tau}_{X, \\mathrm{int}} = \\sum_{j=0}^{j_\\mathrm{max}} \\hat{A}^{XX}_j \\tag{7}\n",
    "$\n",
    "where $\\hat{A}^{XX}_j = \\hat{R}^{XX}_j / \\hat{R}^{XX}_0$ is the estimated ACF. The sum is evaluated up to a maximum number of terms $j_\\mathrm{max}$. This maximum number of terms is a crucial parameter. In the following code cell, $\\hat{\\tau}_{X, \\mathrm{int}}$ is plotted over $j_\\mathrm{max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ACF\n",
    "acf = autocov / autocov[0]\n",
    "\n",
    "# integrate the ACF (suffix _v for vectors)\n",
    "j_max_v = np.arange(1000)\n",
    "tau_int_v = np.zeros(1000)\n",
    "for j_max in j_max_v:\n",
    "    tau_int_v[j_max] = np.sum(acf[:j_max+1])\n",
    "\n",
    "# plot\n",
    "plt.plot(j_max_v,tau_int_v)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"sum length $j_\\mathrm{max}$\")\n",
    "plt.ylabel(r\"$\\tau_{X, \\mathrm{int}}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-credits",
   "metadata": {},
   "source": [
    "The integrated autocorrelation time seems to quickly reach a plateau at a $j_\\mathrm{max}$ of around 20. Further summation over the noisy tail of the ACF results in a random-walky behaviour. And for even larger $j_\\mathrm{max}$, the small unknown bias of the ACF starts to accumulate, which is clearly unwanted. Thus, we have to find a good point to cut off the sum. There have a number of ways been suggested to determine a reasonable value for $j_\\mathrm{max}$. Here, we demonstrate the one by A. Sokal <a href='#[1]'>[1]</a>, who states that it performs well if there are at least 1000 samples in the time series. We take the smallest $j_\\mathrm{max}$, for which the following inequality holds:\n",
    "$\n",
    "j_\\mathrm{max} \\geq C \\times \\tau_{X, \\mathrm{int}}(j_\\mathrm{max}) \\tag{8}\n",
    "$\n",
    "where $C$ is a constant of about 5, or higher if convergence of is slow (up to 10). In the following code cell, we plot the left side against the right side, and determine $j_\\mathrm{max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-software",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C = 5.0\n",
    "\n",
    "# determine j_max\n",
    "j_max = 0\n",
    "while j_max < C*tau_int_v[j_max]:\n",
    "    j_max += 1\n",
    "\n",
    "# plot\n",
    "plt.plot(j_max_v,C*tau_int_v)\n",
    "plt.plot(j_max_v, j_max_v)\n",
    "plt.plot([j_max],[j_max],\"ro\")\n",
    "plt.xscale(\"log\")\n",
    "plt.ylim((0,50))\n",
    "plt.xlabel(r\"sum length $j_\\mathrm{max}$\")\n",
    "plt.ylabel(r\"$\\tau_{X, \\mathrm{int}}$\")\n",
    "plt.show()\n",
    "\n",
    "print(\"j_max = {}\".format(j_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-manner",
   "metadata": {},
   "source": [
    "Using this value of $j_\\mathrm{max}$, we can finish the calculations of the integrated autocorrelation time $\\tau_{X, \\mathrm{int}}$. Using equation (5), finally, the SEM can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_int = tau_int_v[j_max]\n",
    "print(\"Integrated autocorrelation time: {:.2f} time steps\\n\".format(tau_int))\n",
    "\n",
    "N_eff = N_SAMPLES / (2 * tau_int)\n",
    "print(\"Effective number of samples: {:.2f}\".format(N_eff))\n",
    "print(\"Original number of samples: {}\".format(N_SAMPLES))\n",
    "print(\"Ratio: {:.4f}\\n\".format(N_eff/N_SAMPLES))\n",
    "\n",
    "sem = np.sqrt(autocov[0]/N_eff)\n",
    "print(\"Standard error of the mean: {:.4f}\".format(sem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-hayes",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise\n",
    "* Write a function called `autocorrelation_analysis`, which takes as arguments\n",
    "  * `data` (a numpy array containing the time series to be analyzed), \n",
    "  * `C` (which is the criterion to find $j_\\mathrm{max}$) and \n",
    "  * `window` (an integer that defines how much of the auto-covariance function is computed during the analysis).\n",
    "  \n",
    "  The function shall return the SEM and print out:\n",
    "  * mean\n",
    "  * SEM\n",
    "  * integrated autocorrelation time\n",
    "  * effective number of samples. \n",
    "  \n",
    "  It should also make a plot of the autocorrelation function and the integrated ACF. You can adapt the other examples and solutions in this notebook for this function.\n",
    "  \n",
    "* Use this function to analyze `time_series_2`.\n",
    "\n",
    "This function can serve as a template for the analysis of your own simulation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-oriental",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "```python\n",
    "def autocorrelation_analysis(data, C, window):\n",
    "    # initial processing\n",
    "    data_size = len(data)\n",
    "    avg = np.average(data)\n",
    "    data_centered = data - avg\n",
    "\n",
    "    # auto-covariance function\n",
    "    autocov = np.empty(window)\n",
    "    for j in range(window):\n",
    "        autocov[j] = np.dot(data_centered[:data_size-j], data_centered[j:])\n",
    "    autocov /= data_size\n",
    "    \n",
    "    # autocorrelation function\n",
    "    acf = autocov / autocov[0]\n",
    "\n",
    "    # integrate autocorrelation function\n",
    "    j_max_v = np.arange(window)\n",
    "    tau_int_v = np.zeros(window)\n",
    "    for j_max in j_max_v:\n",
    "        tau_int_v[j_max] = np.sum(acf[:j_max+1])\n",
    "    \n",
    "    # find j_max\n",
    "    j_max = 0\n",
    "    while j_max < C*tau_int_v[j_max]:\n",
    "        j_max += 1\n",
    "    \n",
    "    # wrap it up\n",
    "    tau_int = tau_int_v[j_max]\n",
    "    N_eff = data_size / (2 * tau_int)\n",
    "    sem = np.sqrt(autocov[0]/N_eff)\n",
    "\n",
    "    # create ACF plot\n",
    "    plt.gca().axhline(0, color=\"gray\",linewidth=1)\n",
    "    plt.plot(acf)\n",
    "    plt.title(\"Autocorrelation function\")\n",
    "    plt.xlabel(\"lag time\")\n",
    "    plt.show()\n",
    "    \n",
    "    # create integrated ACF plot\n",
    "    plt.plot(j_max_v,C*tau_int_v)\n",
    "    plt.ylim(plt.gca().get_ylim()) # explicitly keep the limits of the first plot\n",
    "    plt.plot(j_max_v, j_max_v)\n",
    "    plt.plot([j_max],[j_max],\"ro\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(r\"sum length $j_\\mathrm{max}$\")\n",
    "    plt.ylabel(r\"$\\tau_{X, \\mathrm{int}}$\")\n",
    "    plt.title(\"\")\n",
    "    plt.show()\n",
    "    \n",
    "    # print out stuff\n",
    "    print(\"Mean value: {:.4f}\".format(avg))\n",
    "    print(\"Standard error of the mean: {:.4f}\".format(sem))\n",
    "    print(\"Integrated autocorrelation time: {:.2f} time steps\".format(tau_int))\n",
    "    print(\"Effective number of samples: {:.2f}\".format(N_eff))\n",
    "\n",
    "    return sem\n",
    "\n",
    "\n",
    "sem_2 = autocorrelation_analysis(time_series_2, 5, 10000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-calibration",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "alternative-wheel",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise\n",
    "Interpret the results of the analysis of `time_series_2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-cricket",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "**Interpretation of the analysis**\n",
    "\n",
    "Even though the autocorrelation analysis spits out a number for the SEM, it cannot be trusted. The integrated ACF shows that in the noise tail of the ACF, there is a negative bias, which pulls down the integrated ACF. This means that the ACF has not properly decayed to zero. The only possibility to get better statistics is to simulate for a longer time. Since the autocorrelation time is very long, it siffuces to store a lot less samples during simulation. The sampling interval could be chosen to be 100 times larger and still capture the statistics sufficiently well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-armstrong",
   "metadata": {},
   "source": [
    "## References\n",
    "<a id='[1]'></a>[1] A. Sokal.  <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.49.4444\">Monte Carlo Methods in Statistical Mechanics: Foundations and New Algorithms Note to the Reader</a> , 1996"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}